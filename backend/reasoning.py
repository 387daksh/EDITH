from backend.vector_store import query_documents
import os
import re
import hashlib
from groq import Groq

# Lazy-loaded globals
_groq_client = None
_dep_graph = None

# ==================== OPTIMIZATION 1: Query Cache ====================
# Caches question->answer pairs to skip LLM calls on repeated questions
# Impact: 100% token savings on cache hits, zero quality impact
_query_cache = {}
CACHE_ENABLED = True

def get_cached_answer(question: str):
    """Check if we have a cached answer for this question."""
    if not CACHE_ENABLED:
        return None
    # Normalize question for better cache hits
    key = hashlib.md5(question.lower().strip().encode()).hexdigest()
    return _query_cache.get(key)

def cache_answer(question: str, answer: str):
    """Cache an answer for future use."""
    if CACHE_ENABLED:
        key = hashlib.md5(question.lower().strip().encode()).hexdigest()
        _query_cache[key] = answer

# ==================== END CACHE ====================

def get_groq_client():
    global _groq_client
    if _groq_client is None:
        api_key = os.environ.get("GROQ_API_KEY")
        if not api_key:
            raise ValueError("GROQ_API_KEY environment variable not set. Please add it to your .env file.")
        _groq_client = Groq(api_key=api_key)
    return _groq_client

def get_dep_graph():
    global _dep_graph
    if _dep_graph is None:
        from backend.graph import DependencyGraph
        _dep_graph = DependencyGraph()
        graph_path = "data/dependency_graph.gml"
        if os.path.exists(graph_path):
            _dep_graph.load(graph_path)
    return _dep_graph

# ==================== OPTIMIZATION 2: Smart Truncation ====================
# Keeps beginning (signatures) + end (returns) instead of just cutting
# Impact: Zero quality loss, preserves important code context
def smart_truncate(text: str, max_length: int = 2500) -> str:
    """Truncate keeping beginning and end portions (preserves function signatures + return statements)."""
    if len(text) <= max_length:
        return text
    
    # Keep 60% from beginning (function signatures, imports) and 40% from end (return statements)
    head_size = int(max_length * 0.6)
    tail_size = int(max_length * 0.35)
    
    return text[:head_size] + "\n\n...(middle section truncated for brevity)...\n\n" + text[-tail_size:]

# ==================== END SMART TRUNCATION ====================

# --- TOOLS ---
def search_code(query: str):
    """Semantic search for code snippets with deduplication."""
    results = query_documents(query, n_results=8)
    output = []
    
    # ==================== OPTIMIZATION 3: Deduplicate Results ====================
    # Removes duplicate chunks from same source file
    # Impact: Positive - improves focus, reduces noise
    seen_sources = set()
    
    if results['documents'] and results['documents'][0]:
        for i, doc in enumerate(results['documents'][0]):
            meta = results['metadatas'][0][i]
            source = meta.get('source', 'unknown')
            
            # Skip if we already have a chunk from this file
            if source in seen_sources:
                continue
            seen_sources.add(source)
            
            # Smart truncate preview
            preview = smart_truncate(doc, max_length=400)
            output.append(f"[{source}]\n{preview}\n")
    
    # ==================== OPTIMIZATION 4: Pre-filter Empty Results ====================
    # If no results found, don't waste LLM tokens on follow-up
    # Impact: Positive - avoids hallucination
    if not output:
        return "No relevant code found for this query. Try different keywords or check if the repository was ingested."
    
    return "\n".join(output)

def read_file(file_path: str):
    """Reads a specific file from the repo with smart truncation."""
    # Search for the file in data/repos
    for root, dirs, files in os.walk("data/repos"):
        for file in files:
            full_path = os.path.join(root, file).replace("\\", "/")
            if file_path in full_path:
                try:
                    with open(os.path.join(root, file), 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                    # Use smart truncation instead of simple cut
                    return smart_truncate(content, max_length=4000)
                except Exception as e:
                    return f"Error reading file: {e}"
    return "File not found. Try using search_code to find the correct path."

def get_dependencies(file_path: str):
    """Returns files that this file imports."""
    deps = get_dep_graph().get_dependencies(file_path)
    return ", ".join(deps) if deps else "No dependencies found."

def get_dependents(file_path: str):
    """Returns files that import this file."""
    deps = get_dep_graph().get_dependents(file_path)
    return ", ".join(deps) if deps else "No dependents found."

# --- AGENT PROMPT (Slightly optimized - removed redundant example) ---
SYSTEM_PROMPT = """You are EDITH, a code analysis AI. Answer questions by reading actual code.

RULES:
1. NEVER guess. ONLY answer based on code you have READ.
2. Use read_file to see code before giving Final Answer.
3. Include file names, function names, and code snippets.

TOOLS:
- search_code(query): Find relevant code. USE FIRST.
- read_file(path): Read file content. USE BEFORE ANSWERING.
- get_dependencies(path): What does this file import?
- get_dependents(path): What imports this file?

FORMAT:
Thought: <reasoning>
Action: [tool_name: argument]

After observations:
Final Answer: <answer WITH code evidence>
"""

def answer_question(question: str) -> str:
    # ==================== OPTIMIZATION 5: Check Cache First ====================
    # Returns cached answer if available (0 tokens used!)
    cached = get_cached_answer(question)
    if cached:
        print("üì¶ Cache hit! Returning cached answer.", flush=True)
        return cached
    
    client = get_groq_client()
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": question}
    ]
    
    all_observations = []
    steps = 0
    max_steps = 6  # Reduced from 10 - most queries resolve in 2-4 steps
    
    while steps < max_steps:
        try:
            chat = client.chat.completions.create(
                messages=messages,
                model="llama-3.3-70b-versatile",
                temperature=0,
                stop=["Observation:"]
            )
        except Exception as e:
            return f"LLM Error: {e}"
            
        response_text = chat.choices[0].message.content.strip()
        messages.append({"role": "assistant", "content": response_text})
        
        # ==================== OPTIMIZATION 6: Early Exit on Final Answer ====================
        # Stop immediately when we have an answer
        if "Final Answer:" in response_text:
            answer = response_text.split("Final Answer:")[-1].strip()
            # Cache this answer for future use
            cache_answer(question, answer)
            return answer
            
        # Parse Action - more flexible regex
        action_match = re.search(r"Action:\s*\[?(\w+)[:]?\s*(.+?)\]?$", response_text, re.MULTILINE)
        if action_match:
            tool = action_match.group(1).strip()
            arg = action_match.group(2).strip().strip(']').strip('"\'')
            
            print(f"ü§ñ Step {steps+1}: {tool}('{arg}')", flush=True)
            
            # Execute Tool
            observation = "Error: Unknown tool. Available: search_code, read_file, get_dependencies, get_dependents"
            if tool == "search_code":
                observation = search_code(arg)
            elif tool == "read_file":
                observation = read_file(arg)
            elif tool == "get_dependencies":
                observation = get_dependencies(arg)
            elif tool == "get_dependents":
                observation = get_dependents(arg)
                
            # Smart truncate observation (already done in tools, but safety check)
            observation = smart_truncate(observation, max_length=2500)
                
            all_observations.append(f"[{tool}({arg})]: {observation[:150]}...")
            messages.append({"role": "user", "content": f"Observation: {observation}"})
            
            # ==================== OPTIMIZATION 7: Trim Message History ====================
            # Keep only last 6 messages to prevent context bloat
            if len(messages) > 8:
                # Keep system prompt + last 5 turns
                messages = [messages[0]] + messages[-6:]
        else:
            # Force a conclusion if format is wrong
            messages.append({"role": "user", "content": "Please provide 'Final Answer:' now based on what you've seen."})
            
        steps += 1
    
    # If we hit the limit, return what we have
    answer = f"Based on my exploration:\n\n{response_text}"
    cache_answer(question, answer)
    return answer

# Utility function to clear cache if needed
def clear_cache():
    """Clear the query cache."""
    global _query_cache
    _query_cache = {}
    print("üóëÔ∏è Query cache cleared.", flush=True)
